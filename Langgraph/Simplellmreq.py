# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ao0rHBY9rQJftj62I83BT88xQBPvTMeD
"""
from langgraph.graph import StateGraph ,START, END
from langchain_google_genai import ChatGoogleGenerativeAI
# from langgraph.graph import StateGraph
from typing import TypedDict
from dotenv import load_dotenv
# load_dotenv("langgraph\.env")

import os
os.environ["GOOGLE_API_KEY"] = "Api-Key"
 

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
# result = model.invoke("HOw are you?")
# print(result.content)

class LLMstate(TypedDict):
    question: str
    answer: str
    
def llm_qa(state: LLMstate) -> LLMstate:
    #extract Queston
    question = state["question"]
    #form a prompt
    prompt = f"""You are a helpful assistant. Answer the question {question}."""
    # Invoke the model and ask que
    answer = model.invoke(prompt).content
    # update answer in state
    state['answer'] = answer
    return state

# Create Graph
graph= StateGraph(LLMstate)
# Add states
graph.add_node("LLM_QA", llm_qa)
# Define start and end states
graph.add_edge(START, "LLM_QA")
graph.add_edge("LLM_QA", END)
# compile
workflow=graph.compile()
# Run the graph
initial_state = {
    "question": "What is the capital of India?"}

final_state = workflow.invoke(initial_state)
print(final_state)